{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "RNN_and_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxUSOU49ga1b"
      },
      "source": [
        "\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltj1je1fp5rO"
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random \n",
        "import sys\n",
        "import os\n",
        "import io "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw7a4NQpJL8i"
      },
      "source": [
        "#https://www.gnu.org/software/sed/manual/sed.html\n",
        "#http://www.gutenberg.org/ebooks/100\n",
        "#http://queirozf.com/entries/sed-examples-search-and-replace-on-linux\n",
        "#https://www.quora.com/What-are-differences-between-update-rules-like-AdaDelta-RMSProp-AdaGrad-and-AdaM?share=1\n",
        "#https://stackoverflow.com/questions/46437761/codecs-openutf-8-fails-to-read-plain-ascii-file/46438434#46438434"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MovdHwpULy2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b33b3828-a3cf-4080-9e16-b888ce0dacf7"
      },
      "source": [
        "#Load Data url\n",
        "\n",
        "path = get_file('100-0.txt', origin='https://www.gutenberg.org/files/100/100-0.txt')\n",
        "\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "  text = f.read().lower()\n",
        "  print('corpus length', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
            "5783552/5777367 [==============================] - 4s 1us/step\n",
            "corpus length 5573152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVhO3NpdMkGa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ab611df-a9e4-4912-c640-3de324e99f61"
      },
      "source": [
        "#Unique Chars as list\n",
        "chars = sorted(list(set(text)))\n",
        "print('Total Unique Chars:', len(chars))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Unique Chars: 79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_8WCLBYNfUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d1a88bc9-f86f-4809-da27-8af2ebab97f8"
      },
      "source": [
        "# lookup tables\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "#create the sequence data\n",
        "maxlen = 40\n",
        "step = 2\n",
        "\n",
        "sentences = []  # Each element is 40 chars long\n",
        "next_chars = [] # One element for each sequence\n",
        "\n",
        "encoded = [char_indices[c] for c in text]\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "print('sequences:', len(sentences))\n",
        "\n",
        "#Create X and y:\n",
        "\n",
        "\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "    \n",
        "print('Vectorization')\n",
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences: 2786556\n",
            "Vectorization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2786556, 40, 79), (2786556, 79))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWEroGaVblfC"
      },
      "source": [
        "##RNN/LSTM Sentiment Classification with Keras\n",
        "Rmsprop IS uses a moving average of squared gradients to normalize the gradient itself. That has an effect of balancing the step size — decrease the step for large gradient to avoid exploding, and increase the step for small gradient to avoid vanishing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5YiJByZKh8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "a01f7835-3e54-48bb-c5a7-116bc960ddd4"
      },
      "source": [
        "#Build Model a single LSTM (Long Short Term Memory (LSTM))\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)))) #Long Short Term Memory\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               106496    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 79)                10191     \n",
            "=================================================================\n",
            "Total params: 116,687\n",
            "Trainable params: 116,687\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK6a9DQClvQT"
      },
      "source": [
        "#Helper Functions\n",
        "\n",
        "Return a probability distribution, and this return the maximun value, which is the most likely character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC44u2jTKiFN"
      },
      "source": [
        "#Helper function to sample an index from a probability array\n",
        "\n",
        "def sample(preds):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / 1\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds/np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPbv9jgcrtU"
      },
      "source": [
        "#Fuction invoked at end of each epoch, Prints generated text.\n",
        "def on_epoch_end(epoch, _):\n",
        "       \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = indices_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLxz0RJhZUJa"
      },
      "source": [
        "#FIT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcg8l6cFWtTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cce4173e-5e59-4f52-bef4-4e28684ac847"
      },
      "source": [
        "#Fit Model:\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=1024,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2721/2722 [============================>.] - ETA: 0s - loss: 2.2745\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \",\n",
            "while you perform your antic round;\n",
            "th\"\n",
            ",\n",
            "while you perform your antic round;\n",
            "thouch me they, fors thous\n",
            "  me oe to mim it your my ho. opt monce poretr’t but asarn,\n",
            "    feer wear\n",
            "  a juki! sherest in i my fore nkere it my thou in wather besteblont bup ow af erofrigh the se, and the berspenithing\n",
            "     i hath soughind\n",
            "      exseect dolturgs’s you to will mengen more ofrantige. u tit list the rokes.\n",
            "  horpusco.\n",
            "wall hol of ar thee;\n",
            "\n",
            "canes, aighis. ay uthes matile;\n",
            "\n",
            "forfes loath \n",
            "2722/2722 [==============================] - 46s 17ms/step - loss: 2.2744\n",
            "Epoch 2/10\n",
            "2720/2722 [============================>.] - ETA: 0s - loss: 1.9248\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"the blood.\n",
            "\n",
            "enter a messenger.\n",
            "\n",
            "messenge\"\n",
            "the blood.\n",
            "\n",
            "enter a messenger.\n",
            "\n",
            "messengeds.\n",
            "what his tait head be gond vereme?\n",
            "   rightles, alstet make arighte'd\n",
            "the nay but in heal achingh ut to frimes.\n",
            "\n",
            "\n",
            "a, sirord.\n",
            "is sun, wherland! or is tome, i knem my fricn.\n",
            "\n",
            "upsteagionts.\n",
            "gook'd thy. af hasle the not nay, buk-monesset lisio]\n",
            "   aplect oniess of path me in your star\n",
            "    thouk cosothour himfalen o freat and of meen:\n",
            "    i not the wave i mansery, soon entitstes ons sue is ghat)\n",
            "\n",
            "h\n",
            "2722/2722 [==============================] - 47s 17ms/step - loss: 1.9249\n",
            "Epoch 3/10\n",
            "2719/2722 [============================>.] - ETA: 0s - loss: 1.8116\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"\n",
            "      when i like your favour; for god \"\n",
            "\n",
            "      when i like your favour; for god whrugn.\n",
            "    welen, and be rem geevel clial, i have nother\n",
            "in blast.\n",
            "\n",
            "then spilldes,\n",
            "than than trule do pray as to spall to this hord me\n",
            "gaing; forso atoursp mescoscess of me came strutiedet?\n",
            "  mane. scanforg. andon thy grocedy to befoart's madess.\n",
            "asa wele tale that your duscoms; if the place.\n",
            "bell entelt.\n",
            "\n",
            "bust.\n",
            "\n",
            "leckener mese;\n",
            "lait, it breath by resamin in thou crovent for see,\n",
            "ald you come coms\n",
            "2722/2722 [==============================] - 46s 17ms/step - loss: 1.8116\n",
            "Epoch 4/10\n",
            "2719/2722 [============================>.] - ETA: 0s - loss: 1.7416\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \"fter, and aim\n",
            "      better at me by that\"\n",
            "fter, and aim\n",
            "      better at me by that i. why, for wheret\n",
            "    pleselfoll. need fornzastor. who have more by art to beefe come, on benwers waster.\n",
            "\n",
            "lagio.\n",
            "must me nutren stweet’s it lows settriun; out?\n",
            "\n",
            "lyrene mantus.\n",
            "he contaty, gune you thear but lastio,\n",
            "    auch as to hun; and thus bleschter; what\n",
            "proppencour asseaster. hust haje to art touse, timanems of sie\n",
            "to magnolip for be starn wordhatt of thesender,\n",
            "he\n",
            "hand a troke gallovard \n",
            "2722/2722 [==============================] - 46s 17ms/step - loss: 1.7416\n",
            "Epoch 5/10\n",
            "2720/2722 [============================>.] - ETA: 0s - loss: 1.6902\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"\n",
            "    was to be so unwise to be so kind.\n",
            "\"\n",
            "\n",
            "    was to be so unwise to be so kind.\n",
            "\n",
            "pallina.\n",
            "a quauding her johe for mose;\n",
            "      the bring ford his spill. throwh pellans of night.\n",
            "with as whith of thy countiune: there!\n",
            "  chaino. he prosse be!' this forshal valing or i wouth-warght!\n",
            "the monetus, which awold not iach the help of sown.\n",
            "\n",
            "                              [“the vettraus; all their spears, and swration!\n",
            "  prsacester. cloud compsteltain\n",
            "of opheldon. ’tis i sur. what mardar\n",
            "2722/2722 [==============================] - 46s 17ms/step - loss: 1.6902\n",
            "Epoch 6/10\n",
            "2718/2722 [============================>.] - ETA: 0s - loss: 1.6507\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \". i was mov'd withal.\n",
            "  coriolanus. i da\"\n",
            ". i was mov'd withal.\n",
            "  coriolanus. i dar so pantlius.\n",
            "  alartellis. noith.\n",
            "\n",
            "nyraw.\n",
            "hastlerst, stile a beethelt! you, by shold,\n",
            " [aney._]\n",
            "\n",
            "scening._] when do have mord throow.\n",
            "    lord. in to you chatizel wat’d cersterdes.\n",
            "\n",
            "æslighalus.\n",
            "you sheavable for dine.\n",
            "\n",
            "‘bear muses to well stand of the fell of debay;\n",
            "      thereown the steemus hall haw that to noine. so,\n",
            "sir, and hessel-not are wherefuls, and port'd.\n",
            "  fallemann. railly death, of\n",
            "2722/2722 [==============================] - 45s 17ms/step - loss: 1.6507\n",
            "Epoch 7/10\n",
            "2719/2722 [============================>.] - ETA: 0s - loss: 1.6192\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"en, best safety lies in fear.\n",
            "youth to i\"\n",
            "en, best safety lies in fear.\n",
            "youth to it but but thy hadablen-bad easencien,\n",
            "    onrush them struk', to with licting whoke\n",
            "noy, by his proce-cieutable?\n",
            "[_hullio.__]\n",
            "\n",
            "[_exit._]\n",
            "\n",
            "donnor.\n",
            "thou knwird to a mornigla! if hen the ceest,\n",
            "cut the from the lord mine your. all the mought,\n",
            "have like a dort; hor all you, gragemen?\n",
            "whocket them, the light casm furdion of dour clue.\n",
            "\n",
            " cruth.\n",
            "wolk; i hold, i wiel me eveis the ruch’d to him.\n",
            "\n",
            "         \n",
            "2722/2722 [==============================] - 45s 17ms/step - loss: 1.6192\n",
            "Epoch 8/10\n",
            "2718/2722 [============================>.] - ETA: 0s - loss: 1.5932\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"nd to-night i'll force\n",
            "    the wine peep\"\n",
            "nd to-night i'll force\n",
            "    the wine peepents york the wasel; a told it\n",
            "new it.\n",
            "\n",
            "belia.\n",
            "i spere thou vity a doget conszale armant.\n",
            "you-simer madan. herselesting lock a villle:\n",
            "the shall stear white the beannaty and sinne.\n",
            "    gledly here, britus and we dread right with me;\n",
            "fir the maicbarlfitless and metter offices they are. do thou\n",
            "    pomen in with but the bracken to’d way short-\n",
            "fire are whiss of meas growness thout pernce\n",
            "   weal not\n",
            "2722/2722 [==============================] - 45s 17ms/step - loss: 1.5932\n",
            "Epoch 9/10\n",
            "2721/2722 [============================>.] - ETA: 0s - loss: 1.5714\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \" queen falls._]\n",
            "\n",
            "osric.\n",
            "look to the quee\"\n",
            " queen falls._]\n",
            "\n",
            "osric.\n",
            "look to the queence and ‘antiss, and low my; if wouch. but now shall’s better\n",
            "\n",
            "gaud her raqueence: follow, how villan’d\n",
            "\n",
            "beothist which faomed, you knave friends,\n",
            "we’lr lord it resides vostated to go.\n",
            "\n",
            "truidass.\n",
            "they well, he reserish:in it is thix;\n",
            "and our from give it me die must rack the good be\n",
            "antled doth forture terrotes of more gain\n",
            "and piscoff, who hall-vient hid a say this depright,\n",
            "i wold. mad, diver th\n",
            "2722/2722 [==============================] - 45s 16ms/step - loss: 1.5713\n",
            "Epoch 10/10\n",
            "2720/2722 [============================>.] - ETA: 0s - loss: 1.5532\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"so did it mine;\n",
            "and a most instant tette\"\n",
            "so did it mine;\n",
            "and a most instant tetter with me say to their pans. a kind-of morrow,\n",
            "    not, an our sleect; and hule percen'd. i’ll crave first, they part chank begar\n",
            "my wilt was my let may alm, is not;\n",
            "for i amonershould of femprce, where’s halg he marken no hour.\n",
            "\n",
            "is then.\n",
            "remonish was inquitter’d in vyourjectipp?\n",
            "    as yes's heart, sin, which, why; i thank, for my sliet, where as a townd\n",
            "coll it i penampt in pay agaid,\n",
            "a constinn\n",
            "2722/2722 [==============================] - 46s 17ms/step - loss: 1.5532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0f863a48d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}